name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  lint-and-format:
    runs-on: ubuntu-latest
    name: Lint and Format Check
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install flake8 black isort mypy
        pip install -r requirements.txt
        
    - name: Run Black formatter check
      run: |
        black --check backend/ frontend/ tests/
        
    - name: Run isort import sorting check
      run: |
        isort --check-only backend/ frontend/ tests/
        
    - name: Run flake8 linting
      run: |
        flake8 backend/ frontend/ tests/ --max-line-length=100 --extend-ignore=E203,W503
        
    - name: Run mypy type checking
      run: |
        mypy backend/ --ignore-missing-imports

  security-scan:
    runs-on: ubuntu-latest
    name: Security Scan
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install safety bandit
        pip install -r requirements.txt
        
    - name: Run safety check
      run: |
        safety check --json --output safety-report.json || true
        
    - name: Run bandit security scan
      run: |
        bandit -r backend/ -f json -o bandit-report.json || true
        
    - name: Upload security reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-reports
        path: |
          safety-report.json
          bandit-report.json

  unit-tests:
    runs-on: ubuntu-latest
    name: Unit Tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: test_cholera_ew
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y postgresql-client libpq-dev
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Set up test database
      run: |
        export DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test_cholera_ew"
        export REDIS_URL="redis://localhost:6379"
        export MLFLOW_TRACKING_URI="file:///tmp/mlflow"
        
    - name: Run database migrations
      run: |
        cd backend
        alembic upgrade head
        
    - name: Run unit tests
      run: |
        cd backend
        pytest tests/ -m unit -v --cov=. --cov-report=xml --cov-report=html --junitxml=test-results.xml
        
    - name: Upload coverage reports
      uses: codecov/codecov-action@v3
      with:
        file: ./backend/coverage.xml
        flags: unit-tests
        name: unit-tests-coverage
        
    - name: Upload test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: test-results
        path: |
          backend/test-results.xml
          backend/htmlcov/

  integration-tests:
    runs-on: ubuntu-latest
    name: Integration Tests
    needs: unit-tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-xdist
        
    - name: Run integration tests
      run: |
        cd backend
        pytest tests/ -m integration -v --dist=worksteal --junitxml=integration-test-results.xml
        
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results
        path: backend/integration-test-results.xml

  docker-build:
    runs-on: ubuntu-latest
    name: Docker Build Test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Build backend Docker image
      run: |
        docker build -t cholera-ew-backend:test ./backend
        
    - name: Build frontend Docker image
      run: |
        docker build -t cholera-ew-frontend:test ./frontend
        
    - name: Test Docker Compose
      run: |
        docker-compose config
        docker-compose build --parallel

  ml-model-tests:
    runs-on: ubuntu-latest
    name: ML Model Tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run ML model tests
      run: |
        cd backend
        pytest tests/test_ml_models.py -v --junitxml=ml-test-results.xml
        
    - name: Upload ML test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: ml-test-results
        path: backend/ml-test-results.xml

  data-validation-tests:
    runs-on: ubuntu-latest
    name: Data Validation Tests
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Run data validation tests
      run: |
        cd backend
        pytest tests/ -k "validation" -v --junitxml=validation-test-results.xml
        
    - name: Upload validation test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: validation-test-results
        path: backend/validation-test-results.xml

  performance-tests:
    runs-on: ubuntu-latest
    name: Performance Tests
    if: github.event_name == 'pull_request'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest-benchmark
        
    - name: Run performance tests
      run: |
        cd backend
        pytest tests/ -m "slow" -v --benchmark-only --benchmark-save=performance
        
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-results
        path: backend/.benchmarks/

  api-documentation:
    runs-on: ubuntu-latest
    name: API Documentation Test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Test API documentation generation
      run: |
        cd backend
        python -c "from main import app; print('API docs generated successfully')"
        
    - name: Validate OpenAPI schema
      run: |
        cd backend
        python -c "
        from main import app
        import json
        openapi_schema = app.openapi()
        json.dumps(openapi_schema)  # This will fail if schema is invalid
        print('OpenAPI schema is valid')
        "

  deploy-staging:
    runs-on: ubuntu-latest
    name: Deploy to Staging
    needs: [lint-and-format, unit-tests, integration-tests, docker-build]
    if: github.ref == 'refs/heads/develop' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Deploy to staging
      run: |
        echo "Deploying to staging environment..."
        # Add your staging deployment commands here
        # Example: kubectl apply -f k8s/staging/
        # Example: docker-compose -f docker-compose.staging.yml up -d

  deploy-production:
    runs-on: ubuntu-latest
    name: Deploy to Production
    needs: [lint-and-format, unit-tests, integration-tests, docker-build, ml-model-tests]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Deploy to production
      run: |
        echo "Deploying to production environment..."
        # Add your production deployment commands here
        # Example: kubectl apply -f k8s/production/
        # Example: docker-compose -f docker-compose.prod.yml up -d

  notify:
    runs-on: ubuntu-latest
    name: Notify Results
    needs: [lint-and-format, unit-tests, integration-tests, docker-build, ml-model-tests]
    if: always()
    
    steps:
    - name: Notify success
      if: ${{ needs.lint-and-format.result == 'success' && needs.unit-tests.result == 'success' && needs.integration-tests.result == 'success' && needs.docker-build.result == 'success' }}
      run: |
        echo "✅ All checks passed! The PR is ready for review."
        
    - name: Notify failure
      if: ${{ needs.lint-and-format.result == 'failure' || needs.unit-tests.result == 'failure' || needs.integration-tests.result == 'failure' || needs.docker-build.result == 'failure' }}
      run: |
        echo "❌ Some checks failed. Please review the logs and fix the issues."
        
    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const { lint, unit, integration, docker } = ${{ toJSON(needs) }};
          const allPassed = lint.result === 'success' && unit.result === 'success' && integration.result === 'success' && docker.result === 'success';
          
          const comment = `## CI/CD Pipeline Results
          
          | Check | Status |
          |-------|--------|
          | Lint & Format | ${lint.result === 'success' ? '✅' : '❌'} |
          | Unit Tests | ${unit.result === 'success' ? '✅' : '❌'} |
          | Integration Tests | ${integration.result === 'success' ? '✅' : '❌'} |
          | Docker Build | ${docker.result === 'success' ? '✅' : '❌'} |
          
          ${allPassed ? '🎉 All checks passed! This PR is ready for review.' : '⚠️ Some checks failed. Please review the logs and fix the issues.'}`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });